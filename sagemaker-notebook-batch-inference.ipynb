{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5cac60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "from sagemaker.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "73cfcec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model artifact location by estimator.model_data, or give an S3 key directly\n",
    "model_artifact_s3_location = \"s3://sagemaker-us-east-1-839052460858/huggingface-sdk-extension-2022-12-23-17-31-18-160/output/model.tar.gz\"\n",
    "\n",
    "# Create PyTorchModel from saved model artifact\n",
    "pytorch_model = HuggingFaceModel(\n",
    "    model_data=model_artifact_s3_location,\n",
    "    role=\"arn:aws:iam::839052460858:role/MLOps\",\n",
    "    transformers_version='4.11',\n",
    "    pytorch_version='1.9',\n",
    "    py_version='py38',\n",
    "    #source_dir=\"model-script/\",\n",
    "    #entry_point=\"train.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0ca94a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer from PyTorchModel object\n",
    "transformer = pytorch_model.transformer(instance_count=1, instance_type=\"ml.m5.large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d735ad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "\u001b[34mWarning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:28,980 [INFO ] main com.amazonaws.ml.mms.ModelServer - \u001b[0m\n",
      "\u001b[34mMMS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 2\u001b[0m\n",
      "\u001b[34mMax heap size: 1726 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-mms.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/mms/models\u001b[0m\n",
      "\u001b[34mInitial Models: ALL\u001b[0m\n",
      "\u001b[34mLog dir: null\u001b[0m\n",
      "\u001b[34mMetrics dir: null\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 2\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mPreload model: false\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,070 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,153 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /.sagemaker/mms/models/model --model-name model --preload-model false --tmp-dir /home/model-server/tmp\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,161 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,161 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 32\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,162 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,163 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.8.10\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,166 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,171 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,196 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,196 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,267 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,281 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,284 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:29,314 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:35,671 [INFO ] pool-2-thread-4 ACCESS_LOG - /169.254.255.130:59036 \"GET /ping HTTP/1.1\" 200 38\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:35,689 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:59046 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,255 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000011-00000000-d3d31c8ea3c94754-95274285\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,257 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7905\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,260 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,282 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000011-00000002-b10bfc8ea3c94755-e8f07706\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,283 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 7903\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,283 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,664 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,663 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Prediction error\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,664 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,664 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:59056 \"POST /invocations HTTP/1.1\" 400 6\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,665 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/decoder_encoder.py\", line 167, in decode\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,665 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     decoder = _decoder_map[content_type]\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,666 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - KeyError: 'OCTET_STREAM'\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,666 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,666 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,666 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,667 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,667 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 234, in handle\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,667 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     response = self.transform_fn(self.model, input_data, content_type, accept)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,668 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 188, in transform_fn\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,668 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     processed_data = self.preprocess(input_data, content_type)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,669 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 136, in preprocess\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,669 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     decoded_input_data = decoder_encoder.decode(input_data, content_type)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,669 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/decoder_encoder.py\", line 170, in decode\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,670 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise errors.UnsupportedFormatError(content_type)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,670 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - sagemaker_inference.errors.UnsupportedFormatError: Content type OCTET_STREAM is not supported by this framework.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,671 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,671 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             Please implement input_fn to to deserialize the request data or an output_fn to\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,671 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             serialize the response. For more information, see the SageMaker Python SDK README.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,672 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,672 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,672 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,673 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,674 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/mms/service.py\", line 108, in predict\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,674 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     ret = self._entry_point(input_batch, self.context)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,674 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 243, in handle\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,678 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise PredictionException(str(e), 400)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,678 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException: Content type OCTET_STREAM is not supported by this framework.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,678 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,679 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             Please implement input_fn to to deserialize the request data or an output_fn to\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,679 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             serialize the response. For more information, see the SageMaker Python SDK README. : 400\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,705 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Prediction error\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,706 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,706 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,706 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/decoder_encoder.py\", line 167, in decode\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,707 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:59064 \"POST /invocations HTTP/1.1\" 400 5\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,707 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     decoder = _decoder_map[content_type]\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,708 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - KeyError: 'OCTET_STREAM'\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,708 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,709 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,709 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,709 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,711 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 234, in handle\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,712 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     response = self.transform_fn(self.model, input_data, content_type, accept)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,712 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 188, in transform_fn\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,712 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     processed_data = self.preprocess(input_data, content_type)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,712 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 136, in preprocess\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,713 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     decoded_input_data = decoder_encoder.decode(input_data, content_type)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,716 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/decoder_encoder.py\", line 170, in decode\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,716 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise errors.UnsupportedFormatError(content_type)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,716 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - sagemaker_inference.errors.UnsupportedFormatError: Content type OCTET_STREAM is not supported by this framework.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,716 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,717 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             Please implement input_fn to to deserialize the request data or an output_fn to\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,717 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             serialize the response. For more information, see the SageMaker Python SDK README.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,717 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,718 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,718 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,718 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,719 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/mms/service.py\", line 108, in predict\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,719 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     ret = self._entry_point(input_batch, self.context)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,719 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 243, in handle\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,720 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise PredictionException(str(e), 400)\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,721 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException: Content type OCTET_STREAM is not supported by this framework.\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,721 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,722 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             Please implement input_fn to to deserialize the request data or an output_fn to\u001b[0m\n",
      "\u001b[34m2022-12-26T18:54:37,722 [INFO ] W-model-1-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             serialize the response. For more information, see the SageMaker Python SDK README. : 400\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-12-26T18:54:35.739:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.688:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/state.json: ClientError: 400\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.689:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/state.json: \u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.689:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/state.json: Message:\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.690:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/state.json: {\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.690:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/state.json:   \"code\": 400,\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.690:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/state.json:   \"type\": \"InternalServerException\",\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.691:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/state.json:   \"message\": \"Content type OCTET_STREAM is not supported by this framework.\\n\\n            Please implement input_fn to to deserialize the request data or an output_fn to\\n            serialize the response. For more information, see the SageMaker Python SDK README.\"\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.691:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/state.json: }\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.737:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/dataset_info.json: ClientError: 400\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.737:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/dataset_info.json: \u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.737:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/dataset_info.json: Message:\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.738:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/dataset_info.json: {\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.738:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/dataset_info.json:   \"code\": 400,\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.738:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/dataset_info.json:   \"type\": \"InternalServerException\",\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.739:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/dataset_info.json:   \"message\": \"Content type OCTET_STREAM is not supported by this framework.\\n\\n            Please implement input_fn to to deserialize the request data or an output_fn to\\n            serialize the response. For more information, see the SageMaker Python SDK README.\"\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:37.739:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/dataset_info.json: }\u001b[0m\n",
      "\u001b[32m2022-12-26T18:54:38.156:[sagemaker logs]: sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/dataset.arrow: Too much data for max payload size\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Transform job hf-pytorch-cpu-2022-12-26-18-49-40-368: Failed. Reason: ClientError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13197/512412062.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transformer.transform(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"s3://sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"S3Prefix\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"OCTET_STREAM\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, batch_data_capture_config, wait, logs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_transform_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     def transform_with_monitoring(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/transformer.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   4240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4242\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TransformJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4244\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3601\u001b[0m                     \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3602\u001b[0m                 )\n\u001b[0;32m-> 3603\u001b[0;31m             raise exceptions.UnexpectedStatusException(\n\u001b[0m\u001b[1;32m   3604\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3605\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Transform job hf-pytorch-cpu-2022-12-26-18-49-40-368: Failed. Reason: ClientError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    data=\"s3://sagemaker-us-east-1-839052460858/samples/datasets/imdb/test/\",\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/json\",\n",
    "    wait=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
