{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d79a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "babeb1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::839052460858:role/MLOps\n",
      "sagemaker bucket: sagemaker-us-east-1-839052460858\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a21f8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b20abcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4bbc7ac20845619e5811f6ff1ec8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17610c70478d4e0880ba3c4251ef443f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-14936b1fa1c6aff6.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6690d7f0f4f54d258b67435336ba0cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fdf0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path,fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c32ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'train_batch_size': 32,\n",
    "                 'model_name':'distilbert-base-uncased'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3bdbd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.m5.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.11',\n",
    "                            pytorch_version='1.9',\n",
    "                            py_version='py38',\n",
    "                            role=role,\n",
    "                            image_uri='839052460858.dkr.ecr.us-east-1.amazonaws.com/hf-pytorch-cpu:1.0',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72203fd9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: huggingface-sdk-extension-2022-12-23-11-07-35-169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-23 11:07:35 Starting - Starting the training job...\n",
      "2022-12-23 11:07:50 Starting - Preparing the instances for training......\n",
      "2022-12-23 11:08:53 Downloading - Downloading input data...\n",
      "2022-12-23 11:09:33 Training - Training image download completed. Training in progress.....\u001b[34m2022-12-23 11:09:57,653 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-12-23 11:09:57,654 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2022-12-23 11:09:57,668 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-12-23 11:09:57,670 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2022-12-23 11:09:57,681 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-12-23 11:09:57,683 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2022-12-23 11:09:57,693 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"huggingface-sdk-extension-2022-12-23-11-07-35-169\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-839052460858/huggingface-sdk-extension-2022-12-23-11-07-35-169/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-839052460858/huggingface-sdk-extension-2022-12-23-11-07-35-169/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"huggingface-sdk-extension-2022-12-23-11-07-35-169\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-839052460858/huggingface-sdk-extension-2022-12-23-11-07-35-169/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34m2022-12-23 11:09:57,693 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2022-12-23 11:09:57,693 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py:1533: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\u001b[0m\n",
      "\u001b[34mYou can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m2022-12-23 11:09:59,297 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2022-12-23 11:09:59,298 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 483/483 [00:00<00:00, 616kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]#015Downloading:   4%|▎         | 9.41M/256M [00:00<00:02, 98.6MB/s]#015Downloading:   8%|▊         | 19.2M/256M [00:00<00:02, 101MB/s] #015Downloading:  11%|█▏        | 29.0M/256M [00:00<00:02, 102MB/s]#015Downloading:  15%|█▌        | 38.7M/256M [00:00<00:02, 102MB/s]#015Downloading:  19%|█▉        | 48.6M/256M [00:00<00:02, 102MB/s]#015Downloading:  23%|██▎       | 58.4M/256M [00:00<00:02, 103MB/s]#015Downloading:  27%|██▋       | 68.2M/256M [00:00<00:01, 102MB/s]#015Downloading:  31%|███       | 78.0M/256M [00:00<00:01, 102MB/s]#015Downloading:  34%|███▍      | 87.8M/256M [00:00<00:01, 102MB/s]#015Downloading:  38%|███▊      | 97.5M/256M [00:01<00:01, 102MB/s]#015Downloading:  42%|████▏     | 107M/256M [00:01<00:01, 102MB/s] #015Downloading:  46%|████▌     | 117M/256M [00:01<00:01, 102MB/s]#015Downloading:  50%|████▉     | 127M/256M [00:01<00:01, 101MB/s]#015Downloading:  53%|█████▎    | 136M/256M [00:01<00:01, 100MB/s]#015Downloading:  57%|█████▋    | 146M/256M [00:01<00:01, 100MB/s]#015Downloading:  61%|██████    | 155M/256M [00:01<00:01, 100MB/s]#015Downloading:  65%|██████▍   | 165M/256M [00:01<00:00, 100MB/s]#015Downloading:  68%|██████▊   | 175M/256M [00:01<00:00, 100MB/s]#015Downloading:  72%|███████▏  | 184M/256M [00:01<00:00, 100MB/s]#015Downloading:  76%|███████▌  | 194M/256M [00:02<00:00, 97.6MB/s]#015Downloading:  79%|███████▉  | 203M/256M [00:02<00:00, 97.7MB/s]#015Downloading:  83%|████████▎ | 212M/256M [00:02<00:00, 97.6MB/s]#015Downloading:  87%|████████▋ | 222M/256M [00:02<00:00, 97.4MB/s]#015Downloading:  90%|█████████ | 231M/256M [00:02<00:00, 98.2MB/s]#015Downloading:  94%|█████████▍| 241M/256M [00:02<00:00, 98.6MB/s]#015Downloading:  98%|█████████▊| 250M/256M [00:02<00:00, 99.1MB/s]#015Downloading: 100%|██████████| 256M/256M [00:02<00:00, 100MB/s] \u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 36.4kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 226k/226k [00:00<00:00, 62.7MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 455k/455k [00:00<00:00, 78.6MB/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Num examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 782\n",
      "  Total optimization steps = 782\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m#015  0%|          | 0/782 [00:00<?, ?it/s]#015  0%|          | 1/782 [00:19<4:16:24, 19.70s/it]#015  0%|          | 2/782 [00:39<4:15:16, 19.64s/it]#015  0%|          | 3/782 [00:58<4:14:26, 19.60s/it]#015  1%|          | 4/782 [01:18<4:15:56, 19.74s/it]#015  1%|          | 5/782 [01:38<4:15:42, 19.75s/it]#015  1%|          | 6/782 [01:58<4:15:11, 19.73s/it]#015  1%|          | 7/782 [02:18<4:16:26, 19.85s/it]#015  1%|          | 8/782 [02:38<4:15:48, 19.83s/it]#015  1%|          | 9/782 [03:00<4:26:19, 20.67s/it]#015  1%|▏         | 10/782 [03:20<4:22:18, 20.39s/it]#015  1%|▏         | 11/782 [03:40<4:19:29, 20.19s/it]#015  2%|▏         | 12/782 [04:00<4:20:04, 20.27s/it]#015  2%|▏         | 13/782 [04:20<4:17:49, 20.12s/it]#015  2%|▏         | 14/782 [04:40<4:15:57, 20.00s/it]#015  2%|▏         | 15/782 [05:00<4:16:20, 20.05s/it]#015  2%|▏         | 16/782 [05:20<4:14:51, 19.96s/it]#015  2%|▏         | 17/782 [05:39<4:13:47, 19.91s/it]#015  2%|▏         | 18/782 [05:59<4:13:42, 19.92s/it]#015  2%|▏         | 19/782 [06:19<4:13:30, 19.93s/it]#015  3%|▎         | 20/782 [06:39<4:12:45, 19.90s/it]#015  3%|▎         | 21/782 [06:59<4:12:34, 19.91s/it]#015  3%|▎         | 22/782 [07:19<4:12:28, 19.93s/it]#015  3%|▎         | 23/782 [07:39<4:11:09, 19.85s/it]#015  3%|▎         | 24/782 [07:59<4:12:25, 19.98s/it]#015  3%|▎         | 25/782 [08:19<4:13:28, 20.09s/it]#015  3%|▎         | 26/782 [08:39<4:11:54, 19.99s/it]#015  3%|▎         | 27/782 [08:59<4:11:46, 20.01s/it]#015  4%|▎         | 28/782 [09:19<4:11:23, 20.00s/it]#015  4%|▎         | 29/782 [09:39<4:10:15, 19.94s/it]#015  4%|▍         | 30/782 [09:59<4:09:38, 19.92s/it]#015  4%|▍         | 31/782 [10:19<4:10:11, 19.99s/it]#015  4%|▍         | 32/782 [10:39<4:09:04, 19.93s/it]#015  4%|▍         | 33/782 [10:59<4:08:51, 19.94s/it]#015  4%|▍         | 34/782 [11:19<4:08:44, 19.95s/it]#015  4%|▍         | 35/782 [11:38<4:07:19, 19.87s/it]#015  5%|▍         | 36/782 [11:58<4:06:57, 19.86s/it]#015  5%|▍         | 37/782 [12:18<4:07:51, 19.96s/it]#015  5%|▍         | 38/782 [12:38<4:07:20, 19.95s/it]#015  5%|▍         | 39/782 [12:58<4:06:29, 19.90s/it]#015  5%|▌         | 40/782 [13:18<4:06:53, 19.96s/it]#015  5%|▌         | 41/782 [13:38<4:05:42, 19.90s/it]#015  5%|▌         | 42/782 [13:58<4:05:05, 19.87s/it]#015  5%|▌         | 43/782 [14:18<4:05:50, 19.96s/it]#015  6%|▌         | 44/782 [14:38<4:04:34, 19.88s/it]#015  6%|▌         | 45/782 [14:57<4:03:47, 19.85s/it]#015  6%|▌         | 46/782 [15:18<4:04:58, 19.97s/it]#015  6%|▌         | 47/782 [15:37<4:03:41, 19.89s/it]#015  6%|▌         | 48/782 [15:57<4:02:39, 19.84s/it]#015  6%|▋         | 49/782 [16:17<4:03:51, 19.96s/it]#015  6%|▋         | 50/782 [16:37<4:02:34, 19.88s/it]#015  7%|▋         | 51/782 [16:57<4:01:51, 19.85s/it]#015  7%|▋         | 52/782 [17:17<4:02:42, 19.95s/it]#015  7%|▋         | 53/782 [17:37<4:01:58, 19.92s/it]#015  7%|▋         | 54/782 [17:57<4:01:03, 19.87s/it]#015  7%|▋         | 55/782 [18:17<4:02:07, 19.98s/it]#015  7%|▋         | 56/782 [18:36<4:00:10, 19.85s/it]#015  7%|▋         | 57/782 [18:56<3:58:49, 19.77s/it]#015  7%|▋         | 58/782 [19:16<4:00:10, 19.90s/it]#015  8%|▊         | 59/782 [19:36<3:58:50, 19.82s/it]#015  8%|▊         | 60/782 [19:55<3:57:28, 19.74s/it]#015  8%|▊         | 61/782 [20:15<3:57:42, 19.78s/it]#015  8%|▊         | 62/782 [20:35<3:56:29, 19.71s/it]#015  8%|▊         | 63/782 [20:54<3:54:52, 19.60s/it]#015  8%|▊         | 64/782 [21:14<3:55:24, 19.67s/it]#015  8%|▊         | 65/782 [21:33<3:53:54, 19.57s/it]#015  8%|▊         | 66/782 [21:53<3:53:09, 19.54s/it]#015  9%|▊         | 67/782 [22:13<3:54:34, 19.68s/it]#015  9%|▊         | 68/782 [22:32<3:53:41, 19.64s/it]#015  9%|▉         | 69/782 [22:52<3:52:50, 19.59s/it]#015  9%|▉         | 70/782 [23:12<3:54:15, 19.74s/it]#015  9%|▉         | 71/782 [23:31<3:53:13, 19.68s/it]#015  9%|▉         | 72/782 [23:51<3:52:22, 19.64s/it]#015  9%|▉         | 73/782 [24:11<3:53:48, 19.79s/it]#015  9%|▉         | 74/782 [24:31<3:52:37, 19.71s/it]#015 10%|▉         | 75/782 [24:50<3:51:38, 19.66s/it]#015 10%|▉         | 76/782 [25:10<3:53:22, 19.83s/it]#015 10%|▉         | 77/782 [25:30<3:51:57, 19.74s/it]#015 10%|▉         | 78/782 [25:49<3:51:04, 19.69s/it]#015 10%|█         | 79/782 [26:10<3:52:26, 19.84s/it]#015 10%|█         | 80/782 [26:29<3:50:47, 19.73s/it]#015 10%|█         | 81/782 [26:49<3:49:39, 19.66s/it]#015 10%|█         | 82/782 [27:09<3:51:24, 19.84s/it]#015 11%|█         | 83/782 [27:28<3:49:01, 19.66s/it]#015 11%|█         | 84/782 [27:47<3:47:05, 19.52s/it]#015 11%|█         | 85/782 [28:07<3:48:03, 19.63s/it]#015 11%|█         | 86/782 [28:27<3:46:55, 19.56s/it]#015 11%|█         | 87/782 [28:46<3:46:27, 19.55s/it]#015 11%|█▏        | 88/782 [29:06<3:48:25, 19.75s/it]#015 11%|█▏        | 89/782 [29:26<3:47:13, 19.67s/it]#015 12%|█▏        | 90/782 [29:45<3:46:10, 19.61s/it]#015 12%|█▏        | 91/782 [30:05<3:47:46, 19.78s/it]#015 12%|█▏        | 92/782 [30:25<3:46:17, 19.68s/it]#015 12%|█▏        | 93/782 [30:44<3:45:13, 19.61s/it]#015 12%|█▏        | 94/782 [31:04<3:46:41, 19.77s/it]#015 12%|█▏        | 95/782 [31:24<3:45:30, 19.70s/it]#015 12%|█▏        | 96/782 [31:44<3:45:00, 19.68s/it]#015 12%|█▏        | 97/782 [32:04<3:46:20, 19.83s/it]#015 13%|█▎        | 98/782 [32:23<3:45:10, 19.75s/it]#015 13%|█▎        | 99/782 [32:43<3:44:10, 19.69s/it]#015 13%|█▎        | 100/782 [33:03<3:45:19, 19.82s/it]#015 13%|█▎        | 101/782 [33:23<3:44:12, 19.75s/it]#015 13%|█▎        | 102/782 [33:42<3:43:14, 19.70s/it]#015 13%|█▎        | 103/782 [34:02<3:44:44, 19.86s/it]#015 13%|█▎        | 104/782 [34:22<3:43:31, 19.78s/it]#015 13%|█▎        | 105/782 [34:42<3:42:32, 19.72s/it]#015 14%|█▎        | 106/782 [35:02<3:43:47, 19.86s/it]#015 14%|█▎        | 107/782 [35:21<3:42:29, 19.78s/it]#015 14%|█▍        | 108/782 [35:41<3:41:32, 19.72s/it]#015 14%|█▍        | 109/782 [36:01<3:42:54, 19.87s/it]#015 14%|█▍        | 110/782 [36:21<3:41:42, 19.80s/it]#015 14%|█▍        | 111/782 [36:40<3:40:37, 19.73s/it]#015 14%|█▍        | 112/782 [37:01<3:41:47, 19.86s/it]#015 14%|█▍        | 113/782 [37:20<3:40:30, 19.78s/it]#015 15%|█▍        | 114/782 [37:40<3:39:30, 19.72s/it]#015 15%|█▍        | 115/782 [38:00<3:40:22, 19.82s/it]#015 15%|█▍        | 116/782 [38:19<3:39:12, 19.75s/it]#015 15%|█▍        | 117/782 [38:39<3:38:14, 19.69s/it]#015 15%|█▌        | 118/782 [38:59<3:38:13, 19.72s/it]#015 15%|█▌        | 119/782 [39:19<3:38:09, 19.74s/it]#015 15%|█▌        | 120/782 [39:38<3:37:44, 19.74s/it]#015 15%|█▌        | 121/782 [39:58<3:37:17, 19.72s/it]#015 16%|█▌        | 122/782 [40:18<3:38:22, 19.85s/it]#015 16%|█▌        | 123/782 [40:38<3:37:04, 19.76s/it]#015 16%|█▌        | 124/782 [40:57<3:36:05, 19.70s/it]#015 16%|█▌        | 125/782 [41:17<3:36:44, 19.79s/it]#015 16%|█▌        | 126/782 [41:37<3:35:28, 19.71s/it]#015 16%|█▌        | 127/782 [41:56<3:34:39, 19.66s/it]#015 16%|█▋        | 128/782 [42:16<3:35:38, 19.78s/it]#015 16%|█▋        | 129/782 [42:36<3:34:22, 19.70s/it]#015 17%|█▋        | 130/782 [42:55<3:33:24, 19.64s/it]#015 17%|█▋        | 131/782 [43:15<3:34:16, 19.75s/it]#015 17%|█▋        | 132/782 [43:35<3:33:03, 19.67s/it]#015 17%|█▋        | 133/782 [43:54<3:31:35, 19.56s/it]#015 17%|█▋        | 134/782 [44:14<3:31:58, 19.63s/it]#015 17%|█▋        | 135/782 [44:33<3:30:26, 19.52s/it]#015 17%|█▋        | 136/782 [44:53<3:29:31, 19.46s/it]#015 18%|█▊        | 137/782 [45:12<3:30:17, 19.56s/it]#015 18%|█▊        | 138/782 [45:31<3:28:41, 19.44s/it]#015 18%|█▊        | 139/782 [45:51<3:27:39, 19.38s/it]#015 18%|█▊        | 140/782 [46:11<3:28:49, 19.52s/it]#015 18%|█▊        | 141/782 [46:30<3:28:27, 19.51s/it]#015 18%|█▊        | 142/782 [46:50<3:28:11, 19.52s/it]#015 18%|█▊        | 143/782 [47:10<3:30:16, 19.74s/it]#015 18%|█▊        | 144/782 [47:29<3:29:33, 19.71s/it]#015 19%|█▊        | 145/782 [47:49<3:28:42, 19.66s/it]#015 19%|█▊        | 146/782 [48:09<3:30:00, 19.81s/it]#015 19%|█▉        | 147/782 [48:29<3:28:49, 19.73s/it]#015 19%|█▉        | 148/782 [48:48<3:27:51, 19.67s/it]#015 19%|█▉        | 149/782 [49:08<3:29:17, 19.84s/it]#015 19%|█▉        | 150/782 [49:28<3:27:49, 19.73s/it]#015 19%|█▉        | 151/782 [49:47<3:26:46, 19.66s/it]#015 19%|█▉        | 152/782 [50:08<3:27:37, 19.77s/it]#015 20%|█▉        | 153/782 [50:27<3:25:47, 19.63s/it]#015 20%|█▉        | 154/782 [50:46<3:24:16, 19.52s/it]#015 20%|█▉        | 155/782 [51:06<3:25:53, 19.70s/it]#015 20%|█▉        | 156/782 [51:26<3:24:59, 19.65s/it]#015 20%|██        | 157/782 [51:45<3:24:29, 19.63s/it]#015 20%|██        | 158/782 [52:05<3:25:54, 19.80s/it]#015 20%|██        | 159/782 [52:25<3:24:32, 19.70s/it]#015 20%|██        | 160/782 [52:44<3:23:41, 19.65s/it]#015 21%|██        | 161/782 [53:05<3:25:07, 19.82s/it]#015 21%|██        | 162/782 [53:24<3:23:56, 19.74s/it]#015 21%|██        | 163/782 [53:44<3:22:59, 19.68s/it]#015 21%|██        | 164/782 [54:04<3:24:20, 19.84s/it]#015 21%|██        | 165/782 [54:23<3:22:53, 19.73s/it]#015 21%|██        | 166/782 [54:43<3:21:49, 19.66s/it]#015 21%|██▏       | 167/782 [55:03<3:22:55, 19.80s/it]#015 21%|██▏       | 168/782 [55:22<3:21:10, 19.66s/it]#015 22%|██▏       | 169/782 [55:42<3:19:47, 19.56s/it]#015 22%|██▏       | 170/782 [56:02<3:21:09, 19.72s/it]#015 22%|██▏       | 171/782 [56:21<3:20:12, 19.66s/it]#015 22%|██▏       | 172/782 [56:41<3:19:19, 19.61s/it]#015 22%|██▏       | 173/782 [57:01<3:20:40, 19.77s/it]#015 22%|██▏       | 174/782 [57:20<3:19:29, 19.69s/it]#015 22%|██▏       | 175/782 [57:40<3:18:36, 19.63s/it]#015 23%|██▎       | 176/782 [58:00<3:19:02, 19.71s/it]#015 23%|██▎       | 177/782 [58:19<3:18:15, 19.66s/it]#015 23%|██▎       | 178/782 [58:39<3:17:18, 19.60s/it]#015 23%|██▎       | 179/782 [58:59<3:17:25, 19.64s/it]#015 23%|██▎       | 180/782 [59:18<3:17:16, 19.66s/it]#015 23%|██▎       | 181/782 [59:38<3:15:51, 19.55s/it]#015 23%|██▎       | 182/782 [59:57<3:15:08, 19.51s/it]#015 23%|██▎       | 183/782 [1:00:17<3:16:44, 19.71s/it]#015 24%|██▎       | 184/782 [1:00:37<3:15:53, 19.65s/it]#015 24%|██▎       | 185/782 [1:00:56<3:15:17, 19.63s/it]#015 24%|██▍       | 186/782 [1:01:16<3:16:20, 19.77s/it]#015 24%|██▍       | 187/782 [1:01:36<3:15:18, 19.69s/it]#015 24%|██▍       | 188/782 [1:01:55<3:14:18, 19.63s/it]#015 24%|██▍       | 189/782 [1:02:15<3:15:08, 19.74s/it]#015 24%|██▍       | 190/782 [1:02:35<3:14:05, 19.67s/it]#015 24%|██▍       | 191/782 [1:02:54<3:12:47, 19.57s/it]#015 25%|██▍       | 192/782 [1:03:14<3:13:15, 19.65s/it]#015 25%|██▍       | 193/782 [1:03:33<3:11:40, 19.53s/it]#015 25%|██▍       | 194/782 [1:03:53<3:11:19, 19.52s/it]#015 25%|██▍       | 195/782 [1:04:13<3:12:39, 19.69s/it]#015 25%|██▌       | 196/782 [1:04:32<3:11:47, 19.64s/it]#015 25%|██▌       | 197/782 [1:04:52<3:11:09, 19.61s/it]#015 25%|██▌       | 198/782 [1:05:12<3:12:02, 19.73s/it]#015 25%|██▌       | 199/782 [1:05:32<3:11:06, 19.67s/it]#015 26%|██▌       | 200/782 [1:05:51<3:10:26, 19.63s/it]#015 26%|██▌       | 201/782 [1:06:11<3:11:24, 19.77s/it]#015 26%|██▌       | 202/782 [1:06:31<3:10:14, 19.68s/it]#015 26%|██▌       | 203/782 [1:06:50<3:08:36, 19.54s/it]#015 26%|██▌       | 204/782 [1:07:10<3:09:35, 19.68s/it]#015 26%|██▌       | 205/782 [1:07:29<3:08:38, 19.62s/it]#015 26%|██▋       | 206/782 [1:07:49<3:08:00, 19.58s/it]#015 26%|██▋       | 207/782 [1:08:09<3:09:42, 19.80s/it]#015 27%|██▋       | 208/782 [1:08:29<3:08:50, 19.74s/it]#015 27%|██▋       | 209/782 [1:08:48<3:07:53, 19.67s/it]#015 27%|██▋       | 210/782 [1:09:08<3:09:04, 19.83s/it]#015 27%|██▋       | 211/782 [1:09:28<3:07:46, 19.73s/it]#015 27%|██▋       | 212/782 [1:09:48<3:07:01, 19.69s/it]#015 27%|██▋       | 213/782 [1:10:08<3:08:10, 19.84s/it]#015 27%|██▋       | 214/782 [1:10:27<3:07:08, 19.77s/it]#015 27%|██▋       | 215/782 [1:10:47<3:06:05, 19.69s/it]#015 28%|██▊       | 216/782 [1:11:07<3:07:04, 19.83s/it]#015 28%|██▊       | 217/782 [1:11:27<3:05:50, 19.73s/it]#015 28%|██▊       | 218/782 [1:11:46<3:04:50, 19.66s/it]#015 28%|██▊       | 219/782 [1:12:06<3:05:16, 19.74s/it]#015 28%|██▊       | 220/782 [1:12:25<3:04:03, 19.65s/it]#015 28%|██▊       | 221/782 [1:12:45<3:03:14, 19.60s/it]#015 28%|██▊       | 222/782 [1:13:05<3:04:27, 19.76s/it]#015 29%|██▊       | 223/782 [1:13:25<3:03:36, 19.71s/it]#015 29%|██▊       | 224/782 [1:13:44<3:02:46, 19.65s/it]#015 29%|██▉       | 225/782 [1:14:04<3:03:50, 19.80s/it]#015 29%|██▉       | 226/782 [1:14:24<3:03:03, 19.76s/it]#015 29%|██▉       | 227/782 [1:14:43<3:02:06, 19.69s/it]#015 29%|██▉       | 228/782 [1:15:04<3:02:53, 19.81s/it]#015 29%|██▉       | 229/782 [1:15:23<3:01:46, 19.72s/it]#015 29%|██▉       | 230/782 [1:15:43<3:00:49, 19.65s/it]#015 30%|██▉       | 231/782 [1:16:02<3:01:00, 19.71s/it]#015 30%|██▉       | 232/782 [1:16:22<2:59:31, 19.58s/it]#015 30%|██▉       | 233/782 [1:16:41<2:58:12, 19.48s/it]#015 30%|██▉       | 234/782 [1:17:01<2:59:43, 19.68s/it]#015 30%|███       | 235/782 [1:17:21<2:58:56, 19.63s/it]#015 30%|███       | 236/782 [1:17:40<2:58:17, 19.59s/it]#015 30%|███       | 237/782 [1:18:00<2:59:45, 19.79s/it]#015 30%|███       | 238/782 [1:18:20<2:58:41, 19.71s/it]#015 31%|███       | 239/782 [1:18:39<2:57:51, 19.65s/it]#015 31%|███       | 240/782 [1:18:59<2:57:39, 19.67s/it]#015 31%|███       | 241/782 [1:19:19<2:57:29, 19.68s/it]#015 31%|███       | 242/782 [1:19:38<2:56:32, 19.62s/it]#015 31%|███       | 243/782 [1:19:58<2:56:07, 19.61s/it]#015 31%|███       | 244/782 [1:20:18<2:56:45, 19.71s/it]#015 31%|███▏      | 245/782 [1:20:37<2:55:54, 19.65s/it]#015 31%|███▏      | 246/782 [1:20:57<2:55:05, 19.60s/it]#015 32%|███▏      | 247/782 [1:21:17<2:55:59, 19.74s/it]#015 32%|███▏      | 248/782 [1:21:36<2:54:24, 19.60s/it]#015 32%|███▏      | 249/782 [1:21:55<2:53:07, 19.49s/it]#015 32%|███▏      | 250/782 [1:22:15<2:53:38, 19.58s/it]#015 32%|███▏      | 251/782 [1:22:35<2:53:07, 19.56s/it]#015 32%|███▏      | 252/782 [1:22:54<2:52:38, 19.54s/it]#015 32%|███▏      | 253/782 [1:23:14<2:53:53, 19.72s/it]#015 32%|███▏      | 254/782 [1:23:34<2:53:18, 19.69s/it]#015 33%|███▎      | 255/782 [1:23:54<2:52:56, 19.69s/it]#015 33%|███▎      | 256/782 [1:24:14<2:54:03, 19.85s/it]#015 33%|███▎      | 257/782 [1:24:34<2:53:12, 19.79s/it]#015 33%|███▎      | 258/782 [1:24:53<2:52:32, 19.76s/it]#015 33%|███▎      | 259/782 [1:25:13<2:53:36, 19.92s/it]#015 33%|███▎      | 260/782 [1:25:33<2:52:38, 19.84s/it]#015 33%|███▎      | 261/782 [1:25:53<2:51:52, 19.79s/it]#015 34%|███▎      | 262/782 [1:26:13<2:52:53, 19.95s/it]#015 34%|███▎      | 263/782 [1:26:33<2:51:22, 19.81s/it]#015 34%|███▍      | 264/782 [1:26:52<2:50:13, 19.72s/it]#015 34%|███▍      | 265/782 [1:27:12<2:50:59, 19.84s/it]#015 34%|███▍      | 266/782 [1:27:32<2:49:51, 19.75s/it]#015 34%|███▍      | 267/782 [1:27:51<2:48:47, 19.66s/it]#015 34%|███▍      | 268/782 [1:28:11<2:49:36, 19.80s/it]#015 34%|███▍      | 269/782 [1:28:31<2:48:29, 19.71s/it]#015 35%|███▍      | 270/782 [1:28:50<2:47:30, 19.63s/it]#015 35%|███▍      | 271/782 [1:29:12<2:53:24, 20.36s/it]#015 35%|███▍      | 272/782 [1:29:34<2:57:03, 20.83s/it]#015 35%|███▍      | 273/782 [1:29:54<2:53:22, 20.44s/it]#015 35%|███▌      | 274/782 [1:30:14<2:52:30, 20.37s/it]#015 35%|███▌      | 275/782 [1:30:34<2:50:03, 20.12s/it]#015 35%|███▌      | 276/782 [1:30:53<2:48:23, 19.97s/it]#015 35%|███▌      | 277/782 [1:31:13<2:48:39, 20.04s/it]#015 36%|███▌      | 278/782 [1:31:33<2:47:25, 19.93s/it]#015 36%|███▌      | 279/782 [1:31:53<2:46:32, 19.87s/it]#015 36%|███▌      | 280/782 [1:32:13<2:47:12, 19.98s/it]#015 36%|███▌      | 281/782 [1:32:33<2:45:46, 19.85s/it]#015 36%|███▌      | 282/782 [1:32:52<2:44:48, 19.78s/it]#015 36%|███▌      | 283/782 [1:33:12<2:45:46, 19.93s/it]#015 36%|███▋      | 284/782 [1:33:32<2:44:50, 19.86s/it]#015 36%|███▋      | 285/782 [1:33:52<2:44:02, 19.80s/it]#015 37%|███▋      | 286/782 [1:34:12<2:44:43, 19.93s/it]#015 37%|███▋      | 287/782 [1:34:32<2:43:46, 19.85s/it]#015 37%|███▋      | 288/782 \u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23591/2259975092.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   4049\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4051\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4053\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLogState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOB_COMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61d2b4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.m5.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.11',\n",
    "                            pytorch_version='1.9',\n",
    "                            py_version='py38',\n",
    "                            role=role,\n",
    "                            image_uri='839052460858.dkr.ecr.us-east-1.amazonaws.com/hf-pytorch-gpu:latest',\n",
    "                            hyperparameters = hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e75a9a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: huggingface-sdk-extension-2022-12-23-17-31-18-160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-23 17:31:18 Starting - Starting the training job...\n",
      "2022-12-23 17:31:33 Starting - Preparing the instances for training......\n",
      "2022-12-23 17:32:35 Downloading - Downloading input data...\n",
      "2022-12-23 17:32:55 Training - Downloading the training image.....................\n",
      "2022-12-23 17:36:26 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34msed: can't read changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: fatal error: no input files\u001b[0m\n",
      "\u001b[34mcompilation terminated.\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.o: No such file or directory\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-12-23 17:37:06,938 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-12-23 17:37:06,939 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-12-23 17:37:06,946 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-12-23 17:37:06,949 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-12-23 17:37:07,107 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-12-23 17:37:07,116 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-12-23 17:37:07,127 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-12-23 17:37:07,134 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"huggingface-sdk-extension-2022-12-23-17-31-18-160\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-839052460858/huggingface-sdk-extension-2022-12-23-17-31-18-160/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-839052460858/huggingface-sdk-extension-2022-12-23-17-31-18-160/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"huggingface-sdk-extension-2022-12-23-17-31-18-160\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-839052460858/huggingface-sdk-extension-2022-12-23-17-31-18-160/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20220929-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2022-12-23 17:37:10,594 - __main__ - INFO -  loaded train_dataset length is: 25000\u001b[0m\n",
      "\u001b[34m2022-12-23 17:37:10,595 - __main__ - INFO -  loaded test_dataset length is: 10000\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/483 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 483/483 [00:00<00:00, 803kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/256M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 8.78M/256M [00:00<00:02, 92.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 18.0M/256M [00:00<00:02, 94.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 27.1M/256M [00:00<00:02, 95.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 36.3M/256M [00:00<00:02, 95.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 45.7M/256M [00:00<00:02, 96.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 55.0M/256M [00:00<00:02, 97.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 64.3M/256M [00:00<00:02, 97.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 73.7M/256M [00:00<00:01, 97.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 82.9M/256M [00:00<00:01, 97.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 92.2M/256M [00:01<00:01, 97.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 102M/256M [00:01<00:01, 97.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 111M/256M [00:01<00:01, 97.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 120M/256M [00:01<00:01, 97.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 129M/256M [00:01<00:01, 97.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 139M/256M [00:01<00:01, 97.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 148M/256M [00:01<00:01, 97.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 157M/256M [00:01<00:01, 97.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 167M/256M [00:01<00:00, 97.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 176M/256M [00:01<00:00, 97.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 185M/256M [00:02<00:00, 97.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 195M/256M [00:02<00:00, 97.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 204M/256M [00:02<00:00, 98.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▎ | 214M/256M [00:02<00:00, 98.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 223M/256M [00:02<00:00, 97.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 232M/256M [00:02<00:00, 97.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 242M/256M [00:02<00:00, 95.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 251M/256M [00:02<00:00, 94.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 256M/256M [00:02<00:00, 96.7MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 50.7kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/226k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 226k/226k [00:00<00:00, 41.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/455k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 455k/455k [00:00<00:00, 76.7MB/s]\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34mThe following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34mNum examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\u001b[0m\n",
      "\u001b[34mGradient Accumulation steps = 1\n",
      "  Total optimization steps = 782\u001b[0m\n",
      "\u001b[34mTotal optimization steps = 782\u001b[0m\n",
      "\u001b[34m0%|          | 0/782 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-12-23 17:37:14.617 ip-10-0-152-133.ec2.internal:38 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-12-23 17:37:14.756 ip-10-0-152-133.ec2.internal:38 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-12-23 17:37:14.757 ip-10-0-152-133.ec2.internal:38 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-12-23 17:37:14.757 ip-10-0-152-133.ec2.internal:38 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-12-23 17:37:14.758 ip-10-0-152-133.ec2.internal:38 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-12-23 17:37:14.758 ip-10-0-152-133.ec2.internal:38 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m0%|          | 1/782 [00:20<4:30:17, 20.77s/it]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24645/2259975092.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2251\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   4049\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4051\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4053\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLogState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOB_COMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
